# Fast_Lightweight_Hyperparameter-Optimization-
Building accurate models requires right choice of hyperparameters for training procedures (learners), when the training dataset is given. AutoML tools provide APIs to automate the choice, which usually involve many trials of diﬀerent hyperparameters for a given training dataset. Since training and evaluation of complex models can be time and resource consuming, existing AutoML solutions require long time or large resource to produce accurate models for large scale training data. That prevents AutoML to be embedded in a software which needs to repeatedly tune hyperparameters and produce models to be consumed by other components, such as large-scale data systems. We present a fast and lightweight hyperparameter optimization method FLO and use it to build an eﬃcient AutoML solution. Our method optimizes for minimal evaluation cost instead of number of iterations to ﬁnd accurate models. Our main idea is to leverage a holistic consideration of the relations among model complexity, evaluation cost and accuracy. FLO has a strong anytime performance and signiﬁcantly outperforms Bayesian Optimization and random search for hyperparameter tuning on a large open source AutoML Benchmark. Our AutoML solution also outperforms top-ranked AutoML libraries in a majority of the tasks on this benchmark.

Chi Wang∗ & Qingyun Wu† November 13, 2019
∗Microsoft Research, Redmond, WA; email: wang.chi@microsoft.com †University of Virginia, Charlottesville, VA; email: qw2ky@virginia.edu; work done in Microsoft Research
https://arxiv.org/pdf/1911.04706.pdf 
